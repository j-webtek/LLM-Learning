# üìò LLM-Learning 

Welcome to LLM-Learning! This repository is dedicated to offering a curated collection of intriguing research papers, innovative projects, and insightful documents from the world of Large Language Models (LLMs) and beyond. LLMs have transformed the AI landscape with their ability to understand and generate human-like text. Here, we gather some of the most notable works that provide insights into their capabilities, applications, and potential impact. Whether you're a seasoned researcher, a developer looking to harness the power of LLMs, or just a curious enthusiast, there's something for everyone. Dive into the transformative capabilities of LLMs and discover their potential across diverse applications.

---

üìà Repository Statistics

Research Papers: 9

Projects: 10

Links & Articles: 55.

---
üÜï ## Latest Additions **10/04/23**

Recent advancements in AI and technology encompass a range of innovations. Neural networks are being developed inspired by biological self-organizing processes, emphasizing the potential of self-organization. Large language models, such as Llama-2, are demonstrating a deeper understanding of space and time, moving beyond mere statistical learning. New techniques in language modeling introduce "pause tokens" to enhance response generation, showing promise in reasoning and question-answering tasks. The integration of large models in robotics suggests the emergence of adaptable "generalist" robotic policies. Tools like AutoGen are revolutionizing multi-agent LLM applications by optimizing complex workflows. The vector database landscape in 2023 is evolving, with platforms like Milvus and Pinecone leading in various aspects. Lastly, Meta Platforms is venturing into generative AI tools, marking a significant step in AI-driven content creation for advertisers.

### üîó Links & Articles

- **[Picking a vector database: a comparison and guide for 2023](https://benchmark.vectorview.ai/vectordbs.html)**
  - üìù The landscape of vector databases in 2023 is diverse, with each offering unique features catering to different needs. Milvus and Pinecone stand out for their performance and flexibility.

- **[Meta starts rolling out generative AI tools for all advertisers](https://www.reuters.com/technology/meta-starts-rolling-out-generative-ai-tools-all-advertisers-2023-10-04/)**
  - üìù Meta Platforms is introducing generative AI tools for advertisers, marking its initial venture into integrating generative AI technology.

### üìÑ Research Papers

- **Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs**
  - üìú [Read the Paper](https://arxiv.org/abs/2307.08197)
  - üìù This paper explores the creation of neural networks inspired by the self-organizing developmental processes seen in biological nervous systems.

- **Language Models Represent Space and Time**
  - üìú [Read the Paper](https://arxiv.org/abs/2310.02207)
  - üìù Large language models (LLMs) like Llama-2 exhibit capabilities suggesting they learn coherent world models, understanding linear dimensions of space and time.

- **Think before you speak: Training Language Models with Pause Tokens**
  - üìú [Read the Paper](https://arxiv.org/abs/2310.02226)
  - üìù Language models traditionally generate responses token-by-token, but introducing a "pause token" allows the model to process additional computation before finalizing an answer.

### üõ† Projects

- **AutoGen**
  - üåê [Visit the Project](https://microsoft.github.io/autogen/docs/getting-started)
  - üìù AutoGen is a tool designed for building next-gen LLM applications centered on multi-agent conversations, streamlining the orchestration, automation, and optimization of complex LLM workflows.


---

## üöÄ Quick Navigation

1. [Visual Diagrams](#visual-diagrams)
2. [Research Papers](#research-papers)
3. [Projects](#projects)
4. [Links & Articles](#links--articles)
5. [License](#license)

---

## üìä Visual Diagrams

For a pictorial insight:
- [Data to LLM Processes](Data%20to%20LLM%20Processes.pdf)
- [Foundation Models](Foundation%20Models.pdf)
- [LLM Stack](LLM%20Stack.pdf)
- [Multi_Models](Multi_Models.pdf)

---

## üìÑ Research Papers

Dive deep into the world of research with these enlightening papers:

### üíº Applications and Deployment

- **Think before you speak: Training Language Models with Pause Tokens**
  - üìú [Read the Paper](https://arxiv.org/abs/2310.02226)
  - üìù Language models traditionally generate responses token-by-token but introducing a learnable "pause token" allows the model to process additional computation before finalizing an answer.
  - 
- **MotionLM: Multi-Agent Motion Forecasting as Language Modeling**
  - üìú [Read the Paper](https://arxiv.org/abs/2309.16534)
  - üìù The study introduces MotionLM, a model that predicts multi-agent motion by treating trajectories as sequences of discrete motion tokens, akin to a language modeling task.

- **Large Language Models for Compiler Optimization**
  - üìú [Read the Paper](https://arxiv.org/pdf/2309.07062.pdf)
  - üìù A study on the use of Large Language Models for code optimization.

- **PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions**
  - üìú [Read the Paper](https://arxiv.org/pdf/2308.12261.pdf)
  - üìù A method that transforms natural language task descriptions into deployable models.

### üìä Data Handling and Annotation

- **Language Models Represent Space and Time**
  - üìú [Read the Paper](https://arxiv.org/abs/2310.02207)
  - üìù Large language models (LLMs) like Llama-2 exhibit capabilities that suggest they learn coherent world models rather than just superficial statistics.
  - 
- **AnnoLLM: Making Large Language Models to Be Better**
  - üìú [Read the Paper](https://www.semanticscholar.org/reader/70da4fb798a86cbe8cad96c27ced0415885bbd9d)
  - üìù Large language models (LLMs) like GPT-3.5 can serve as effective crowdsourced annotators when given sufficient guidance and example demonstrations.

- **Want To Reduce Labeling Cost? GPT-3 Can Help**
  - üìú [Read the Paper](https://www.semanticscholar.org/reader/70da4fb798a86cbe8cad96c27ced0415885bbd9d)
  - üìù The study explores using GPT-3 as a cost-effective data labeler for training other models in NLP tasks.

- **DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions**
  - üìú [Read the Paper](https://aclanthology.org/2023.acl-long.573.pdf)
  - üìù The study introduces the task of recommending datasets based on a short natural language description of a research idea.

### üé® Graphics and Imaging

- **CoRF: Colorizing Radiance Fields using Knowledge Distillation**
  - üìú [Read the Paper](https://arxiv.org/abs/2309.07668)
  - üìù This research introduces a method to generate colorized novel views from input grayscale multi-view images using Neural Radiance Fields (NeRF). The proposed distillation-based technique effectively transfers color knowledge from 2D colorization methods to the radiance field network, ensuring 3D consistency and producing superior results in both indoor and outdoor scenes compared to other methods.

### üöÄ Retrieval Techniques

- **Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs**
  - üìú [Read the Paper](https://arxiv.org/abs/2307.08197)
  - üìù This paper explores the creation of neural networks inspired by the self-organizing developmental processes seen in biological nervous systems.

- **Precise Zero-Shot Dense Retrieval without Relevance Labels**
  - üìú [Read the Paper](https://aclanthology.org/2023.acl-long.99.pdf)
  - üìù Introduction of Hypothetical Document Embeddings (HyDE) for zero-shot dense retrieval.

---

## üõ†Ô∏è Projects

### üí° Model Optimization & Fine-Tuning

#### Data Cleaning and Labeling
- üåü **Cleanlab** - An aid for cleaning data and labels in ML datasets. [Check it out](https://github.com/cleanlab/cleanlab).

#### Model Adaptation and Fine-Tuning
- üåü **LLaMa-Adapter** - A lightweight adaptation method for fine-tuning Instruction-following and Multi-modal LLaMA models. [Learn more](https://github.com/OpenGVLab/LLaMA-Adapter).
- üåü **axolotl** - A tool designed to streamline the fine-tuning of AI models. [Check it out](https://github.com/OpenAccess-AI-Collective/axolotl).

### üåê Multimodal Models & Applications

#### Curated Lists and Collections
- üåü **Awesome-Multimodal-Large-Language-Models** - A curated list of Multimodal Large Language Models. [Explore here](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models).

#### Frameworks and Systems
- üåü **DreamLLM** - A learning framework focusing on synergy between multimodal comprehension and creation. [Dive in](https://github.com/RunpeiDong/DreamLLM).
- üåü **NExT-GPT** - The first end-to-end MM-LLM for text, image, video, and audio. [Discover more](https://github.com/NExT-GPT/NExT-GPT).

### üõçÔ∏è Collections & Repositories

#### LLM Collections and Demonstrations
- üåü **LargeLanguageModelsProjects** - A collection of llama models in different configurations. [Explore](https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Chat%20with%20Multiple%20Documents/Chat_with_Multiple_Documents_Llama2_OpenAI_Chroma_comp.ipynb).

#### Toolkits and Utilities
- üåü **LiteLLM** - Manages inputs to the provider's completion and embedding endpoints. [Discover](https://github.com/BerriAI/litellm).
- üåü **AutoGPT** - A modular toolkit for AI agents. [Explore on GitHub](https://github.com/Significant-Gravitas/AutoGPT).
- üåü **localGPT** - Interact with documents locally ensuring data privacy. [Check it out on GitHub](https://github.com/PromtEngineer/localGPT).
- üåü **LLM-Finetuning-Hub** - Resources for finetuning LLMs tailored to specific use cases. [Learn more on GitHub](https://github.com/georgian-io/LLM-Finetuning-Hub).
- üåü **chatgpt-history-export-to-md** - Convert your ChatGPT history and data export into neatly formatted Markdown files. It includes YAML headers and a Code interpreter for Advanced Data Analysis. [Check it out on GitHub](https://github.com/mohamed-chs/chatgpt-history-export-to-md).

---

## üîó Links & Articles

### üìà Research & Innovations
- [AutoGen](https://microsoft.github.io/autogen/docs/getting-started): AutoGen is a tool designed for building next-gen LLM applications centered on multi-agent conversations, streamlining the orchestration, automation, and optimization of complex LLM workflows. It offers customizable conversation patterns, a variety of pre-built systems across domains, and an enhanced inference API that integrates features like performance tuning, API unification, and advanced usage patterns.
- [Introduction to Flash Attention: A Breakthrough in Efficient Attention Mechanism](https://medium.com/@sthanikamsanthosh1994/introduction-to-flash-attention-a-breakthrough-in-efficient-attention-mechanism-3eb47e8962c3)
- [Break-A-Scene: Extracting Multiple Concepts from a Single Image](https://omriavrahami.com/break-a-scene/)
- [Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm) ([Research Paper](https://arxiv.org/abs/2309.17453))
- [Introducing Stable LM 3B: Bringing Sustainable, High-Performance Language Models to Smart Devices](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices)
- [Introduction to Flash Attention: A Breakthrough in Efficient Attention Mechanism](https://medium.com/@sthanikamsanthosh1994/introduction-to-flash-attention-a-breakthrough-in-efficient-attention-mechanism-3eb47e8962c3): Flash Attention is a groundbreaking advancement in attention mechanisms, offering a faster and more memory-efficient solution compared to traditional methods.
- [Break-A-Scene: Extracting Multiple Concepts from a Single Image](https://omriavrahami.com/break-a-scene/): Text-to-image model personalization seeks to incorporate user-provided concepts into models for diverse synthesis.
- [Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm) ([Research Paper](https://arxiv.org/abs/2309.17453)): Deploying LLMs in streaming applications presents challenges, including memory consumption and handling longer texts.
- [Introducing Stable LM 3B: Bringing Sustainable, High-Performance Language Models to Smart Devices](https://stability.ai/blog/stable-lm-3b-sustainable-high-performance-language-models-smart-devices): Stable LM 3B, a compact 3 billion parameter language model, operates efficiently on portable devices.
- [MuZero: Mastering Go, chess, shogi and Atari without rules](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules): DeepMind's MuZero represents a significant advancement in AI, mastering games like Go, chess, shogi, and Atari without being pre-informed of the rules, showcasing its capability to strategize in unknown environments. Going beyond previous AI models, MuZero learns only the vital aspects of its environment and combines this with a look-ahead tree search, setting new performance standards while potentially leading to applications in real-world scenarios where rules are undefined.
- [Anand Katti](https://www.linkedin.com/posts/anand-katti-4278637_ai-genai-llm-activity-7097479600368160768-jNzj/) - LLMOps enhances the MLOps framework by introducing LLM-specific tasks.
- [An Introduction to LLMOps: Operationalizing and Managing Large Language Models using Azure ML](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/an-introduction-to-llmops-operationalizing-and-managing-large/ba-p/3910996) - Large language models (LLMs) like GPT-4 have transformed natural language processing with their superior performance, but their real-world deployment requires a systematic approach called LLMOps.
- [GPT-4V (ision) System Card](https://cdn.openai.com/papers/GPTV_System_Card.pdf) - GPT-4 with Vision (GPT-4V) combines text and image processing, expanding the impact of language-only systems.
- [Our Humble Attempt at ‚ÄúHow Much Data Is Needed to Fine-Tune‚Äù](https://barryzhang.substack.com/p/our-humble-attempt-at-fine-tuning) - Researchers experiment with the OpenAI fine-tuning API, revealing that GPT-3.5 can achieve near GPT-4 performance in specialized tasks.
- [Falcon 180B: Can It Run on Your Computer?](https://kaitchup.substack.com/p/falcon-180b-can-it-run-on-your-computer) - The Technology Innovation Institute (TII) introduces Falcon 180B, a massive 180 billion parameter model, emphasizing its memory requirements.

### üåå General AI Insights

#### AI Model Philosophies and Debates
- [Vector database is not a separate database category](https://nextword.substack.com/p/vector-database-is-not-a-separate)
- [First Impressions with GPT-4V(ision)](https://blog.roboflow.com/gpt-4-vision/): OpenAI has introduced GPT-4V, a multimodal extension of the GPT-4 model, allowing users to input images and voice queries. While the model excels at general image questions, providing context-aware answers, it has limitations, including inaccuracies in object detection and occasional "hallucinations" of incorrect information.
- [Why Open Source AI Will Win](https://varunshenoy.substack.com/p/why-open-source-ai-will-win) - The debate between open-source and closed-source AI models.
- [NSA, FBI, and CISA Release Cybersecurity Information Sheet on Deepfake Threats](https://www.cisa.gov/news-events/alerts/2023/09/12/nsa-fbi-and-cisa-release-cybersecurity-information-sheet-deepfake-threats): The NSA, FBI, and CISA have released a Cybersecurity Information Sheet on the rising threat of synthetic media, including deepfakes, highlighting their growing impact on sectors like the NSS, DoD, and national infrastructure. The agencies emphasize the importance of reviewing their recommended steps and best practices to address deepfake threats effectively.

#### AI Technologies and Capabilities
- [Picking a vector database: a comparison and guide for 2023](https://benchmark.vectorview.ai/vectordbs.html): The landscape of vector databases in 2023 is diverse, with each offering unique features catering to different needs. While Milvus stands out in performance and community strength, Pinecone shines in developer experience and hosting; the ideal choice varies based on project specifics, budget, and preferences, with the author leaning towards Pinecone and Milvus for their performance and flexibility.
- [Graph-mining](https://github.com/google/graph-mining): Google's tools for tasks with inherent graph structures, used in products like Search, YouTube, and Maps.
- [OnnxStream](https://github.com/vitoplantamura/OnnxStream/tree/846da873570a737b49154e8f835704264864b0fe): A specialized inference library designed to minimize memory consumption.
- [Released L2E OS v0.1 "Temple DOS" . A new OS on the block! The first OS that boots to AI!](https://twitter.com/VulcanIgnis/status/1708851772435968017): A new operating system that boots directly to an AI interface.
- [Decentralized Artificial Intelligence](https://www.chaos-engineering.dev/p/decentralized-artificial-intelligence): The article discusses the need for decentralization in Large Language Models (LLMs) and AI.
- [Microsoft is going nuclear to power its AI ambitions](https://www.theverge.com/2023/9/26/23889956/microsoft-next-generation-nuclear-energy-smr-job-hiring): Microsoft is exploring the potential of next-generation nuclear reactors to power its data centers and support its AI initiatives.
- [Run any ML model from any programming language](https://carton.run/): Carton is a tool that packages machine learning models with metadata in a zip file without altering the original model, simplifying the model deployment process by automatically selecting the appropriate runner based on the metadata. Once packed, Carton's framework-agnostic API facilitates model inference, with the software built in Rust and offering bindings to multiple languages.
- [The next generation of smart glasses](https://www.meta.com/smart-glasses/): Ray-Ban and Meta collaborate on sunglasses that video record, and image capture, and can be queried via Meta‚Äôs AI
- [Vectorize: a vector database for shipping AI-powered applications to production, fast](https://blog.cloudflare.com/vectorize-vector-database-open-beta/): Vectorize is a new vector database from Cloudflare designed to help machine learning models "remember" and enhance AI-powered applications. It allows developers to build full-stack AI applications on Cloudflare's global network, enhancing semantic search, classification, recommendation, and anomaly detection use-cases. Vectorize is in open beta and integrates with Cloudflare Workers, enabling it to power various applications, including improving LLMs' accuracy and context, and supporting embeddings from platforms like OpenAI and Cohere.
- [Workers AI: serverless GPU-powered inference on Cloudflare‚Äôs global network](https://blog.cloudflare.com/workers-ai/): Cloudflare is launching Workers AI, a platform that allows developers to run AI models using a few lines of code without managing infrastructure, emphasizing accessibility, serverless operation, and privacy. The service offers popular open-source AI models, ensures data privacy by default, and will soon expand its model offerings through a partnership with Hugging Face, with plans for rapid GPU rollout across global data centers by 2024.
- [You can give ChatGPT a picture of your team‚Äôs whiteboarding session and have it write the code for you.](https://twitter.com/mckaywrigley/status/1707101465922453701?s=20): This is absolutely insane.
- [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/): Mistral 7B, a 7.3B parameter model, outperforms various Llama versions on benchmarks, excels in both code and English tasks, and uses efficient attention mechanisms for better performance. Available under the Apache 2.0 license, it's easy to deploy across platforms, and a fine-tuned chat variant surpasses Llama 2 13B chat in performance.
- [AI startup Lamini bets future on AMD's Instinct GPUs](https://www.theregister.com/2023/09/26/amd_instinct_ai_lamini/): AI startup Lamini has chosen to exclusively use AMD's Instinct GPUs for its platform that refines large language models (LLMs), setting itself apart from many competitors that rely on Nvidia's hardware. While Lamini's platform has garnered interest from major companies like Amazon and Walmart, AMD's focus on expanding its software ecosystem and forthcoming hardware upgrades aim to make its AI accelerators more attractive and accessible to developers and businesses.
- [Vicious Self-Degradation](https://twitter.com/8teapi/status/1706520893621784780): When a frequent query is Googled, Quora identifies it, uses ChatGPT to generate an answer that may contain a hallucination, and this ChatGPT-generated response becomes the top Google answer.
- [Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond](https://pytorch.org/blog/inside-the-matrix/): The article introduces "mm", a visualization tool designed to display matrix multiplications (matmuls), which are foundational to machine learning models. Using three-dimensional visualizations, "mm" enables clearer understanding of complex matrix operations and compositions, especially benefiting visual thinkers, and covers various topics from basic matrix operations to the intricate workings of GPT-2 attention heads, demonstrating the benefits of this geometric approach to understanding algebraic properties in matrix computations.
- [Can you beat a stochastic parrot?](https://parrotchess.com/): Play chess against GPT-3.5.

#### Ethical and Societal Implications of AI
- [National Security Agency is starting an artificial intelligence security center](https://apnews.com/article/nsa-artificial-intelligence-security-deepfakes-f9b19dd64890884cc2b0700ddf66e666): The NSA is launching an AI security center to bolster defense and intelligence systems against threats.
- [WebGPU Technical Report](https://chromium.googlesource.com/chromium/src/+/main/docs/security/research/graphics/webgpu_technical_report.md): WebGPU introduces extensive attack surfaces to Chrome's GPU process, including the core WebGPU, third-party Usermode Graphics Drivers, and shader compilers, emphasizing the complexity that may result in vulnerabilities. Despite significant efforts to validate input and extensive fuzzing, there remain concerns about potential vulnerabilities in areas like Dawn's use-after-frees, callbacks, the Chrome Command Buffer, and the SwiftShader JIT compiler, indicating a need for ongoing vigilance and manual audits. assistance 24/7 without human intervention, though human volunteers remain available for more nuanced assistance.
- [Be My AI](https://www.bemyeyes.com/blog/announcing-be-my-ai): Be My Eyes, a platform connecting volunteers with visually impaired users, has integrated an AI feature called "Be My AI" to assist with everyday tasks and is now launching it in an open beta phase for iOS users, with an Android version in the pipeline. "Be My AI" allows users to take photos and receive detailed descriptions, proving invaluable for tasks like reading labels, organizing wardrobes, and getting visual assistance 24/7 without human intervention, though human volunteers remain available for more nuanced assistance.
- [Signal‚Äôs Meredith Whittaker: AI is fundamentally ‚Äòa surveillance technology‚Äô](https://techcrunch.com/2023/09/25/signals-meredith-whittaker-ai-is-fundamentally-a-surveillance-technology/): Meredith Whittaker, Signal's president, emphasized at TechCrunch Disrupt 2023 that AI is fundamentally a surveillance technology, deeply intertwined with the big data and targeting industry dominated by tech giants. While acknowledging not all AI applications are exploitative, she highlighted the inherent surveillance nature of AI and the economic drivers behind facial recognition technology, noting that beneficial uses, like face blurring in Signal's app, are overshadowed by more intrusive applications driven by profit motives.

#### AI in Education
- [Student Use Cases for AI](https://hbsp.harvard.edu/inspiring-minds/student-use-cases-for-ai): Generative AI tools, particularly large language models (LLMs), present both opportunities and challenges in educational settings, offering students and educators unparalleled access to powerful AI systems. As AI becomes increasingly prevalent in classrooms, it's crucial for educators and students to understand the potential benefits, biases, and privacy concerns of AI, and to adopt best practices for interacting with these tools to ensure effective and safe usage.

#### LLM Architectures and Applications
- [OpenAI](https://twitter.com/openai/status/1707077710047216095?s=46&t=Tn3eky5MQ9AEY1npL2msJw): ChatGPT can now browse the internet to provide you with current and authoritative information, complete with direct links to sources. It is no longer limited to data before September 2021.
- [RAG is more than just embedding search](https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/) - Potential of Retrieval Augmented Generation (RAG) in LLMs.
- [Emerging Architectures for LLM Applications](https://a16z.com/emerging-architectures-for-llm-applications/) - The article presents an architecture for applications using large language models (LLMs).
- [Essential Guide to Foundation Models and Large Language Models](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404) - Foundation models explained.

#### Best Practices and Guidelines
- [Practical insights and best practices for Fine Tuned LLM based use cases for Governed Enterprises](https://3ai.in/practical-insights-and-best-practices-for-fine-tuned-llm-based-use-cases-for-governed-enterprises/) - Aditya Khandekar's article on 3AI discusses best practices for deploying Large Language Models (LLMs) in enterprise settings.
- [Why You (Probably) Don‚Äôt Need to Fine-tune an LLM](https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/) - This article guides those building applications with Large Language Models (LLMs), emphasizing that while many consider fine-tuning LLMs to enhance performance, there are often simpler and more effective alternatives.

#### AI in Multimedia and Collaboration
- [Meta starts rolling out generative AI tools for all advertisers](https://www.reuters.com/technology/meta-starts-rolling-out-generative-ai-tools-all-advertisers-2023-10-04/): Meta Platforms is introducing generative AI tools for advertisers, allowing them to create content such as image backgrounds and text variations. This move, marking Meta's initial venture into integrating generative AI technology, will also soon enable businesses to use AI for messaging on platforms like Messenger and WhatsApp.
- [captcha with Bing](https://twitter.com/literallydenis/status/1708283962399846459): I've tried to read the captcha with Bing, and it is possible after some prompt-visual engineering.
- [What codegen is (actually) good for](https://www.figma.com/blog/what-codegen-is-actually-good-for/#aAkZ9): Codegen, the automatic generation of code based on predefined rules, is gaining popularity, with tools ranging from simple code completion in IDEs to advanced AI-driven systems. While many developers use AI for code generation, skepticism remains about its accuracy; instead of viewing codegen as a full replacement, it's best seen as an extension of a developer, assisting in the design-to-development process by suggesting suitable tools, speeding up workflows, and acting as a reference, though it won't entirely replace established team patterns.
- [Introducing New AI Experiences Across Our Family of Apps and Devices](https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/): Meta is introducing a range of AI-driven features, including AI stickers for image editing on apps like Instagram, the advanced conversational assistant 'Meta AI' for multiple platforms, and 28 additional AIs with distinct personalities, some portrayed by celebrities. As they expand AI offerings for businesses and developers, they acknowledge potential challenges and emphasize a cautious approach with built-in safeguards.
- [Getty made an AI generator that only trained on its licensed images](https://www.theverge.com/2023/9/25/23884679/getty-ai-generative-image-platform-launch) - Getty Images collaborated with Nvidia to introduce a tool allowing users to produce images from its extensive library, ensuring full copyright indemnification.
- [ChatGPT can now see, hear, and speak](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak) - ChatGPT introduces new voice and image capabilities, emphasizing user safety, privacy, and understanding model limitations.
- [Expanding access to safer AI with Amazon](https://www.anthropic.com/index/anthropic-amazon) - Amazon's collaboration with Anthropic aims to create industry-leading foundation models and make advanced models like Claude 2 accessible through Amazon Bedrock.

### üõ†Ô∏è Tools & Databases

#### LLM Databases and Search Engines
- [Guide to Chroma DB | A Vector Store for Generative AI LLMs](https://www.analyticsvidhya.com/blog/2023/07/guide-to-chroma-db-a-vector-store-for-your-generative-ai-llms/) - An introduction to Chroma DB, a vector database for LLMs.
- [Build AI search into your applications](https://www.elastic.co/elasticsearch/elasticsearch-relevance-engine) - Introduction to the Elasticsearch Relevance Engine‚Ñ¢ for AI-based search.

#### LLM Integration and APIs
- [Llama API](https://python.langchain.com/docs/integrations/chat/llama_api) - This notebook shows how to use LangChain with LlamaAPI.

#### LLM Toolkits and Platforms
- [Embedchain](https://docs.embedchain.ai/quickstart) - Start building LLM-powered bots in under 30 seconds.
- [Two-Tower Embedding Model](https://www.hopsworks.ai/dictionary/two-tower-embedding-model) - A training approach aligning embeddings from two modalities, like images and text, useful for personalized recommendation systems.

### üìö Tutorials & How-Tos

- [How to make history with LLMs & other generative models](https://leighmariebraswell.substack.com/p/how-to-make-history-with-llms-and): The author discusses the transformative potential of Large Language Models (LLMs), highlighting promising startup ideas in areas like developer tooling and knowledge worker augmentation, while expressing skepticism about general consumer search and some SaaS replacements. On the infrastructure side, running large models locally and providing compute for model training are seen as promising, but ventures into observability and vector databases face more challenges.
- [A poor man's guide to fine-tuning Llama 2](https://duarteocarmo.com/blog/fine-tune-llama-2-telegram): The author details the ease and efficiency of fine-tuning the Llama 2 model to simulate conversations from their personal
- [Building a Knowledge base for custom LLMs](https://cismography.medium.com/building-a-knowledge-base-for-custom-llms-using-langchain-chroma-and-gpt4all-950906ae496d) - How to build a knowledge base for custom LLMs.
- [Fine-Tuning LLMs: LoRA or Full-Parameter?](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) - A comparison of full-parameter fine-tuning and LoRA for Llama 2 models.
- [Building a Scalable Pipeline for LLMs and RAG](https://ai.plainenglish.io/building-a-scalable-pipeline-for-large-language-models-and-rag-an-overview-7cb93a03f657) - Constructing a scalable pipeline for LLMs and RAG.
- [How to run a llama, alpaca, vicuna REST API for AI Discord Bot, Part 1](https://www.youtube.com/watch?v=PMFf9FwPN70&ab_channel=Janek) - How to set up a llama.cpp python binding server to host an API for LLM and how to create a python script for discord bot.
- [A Hackers' Guide to Language Models](https://www.youtube.com/watch?v=jkrNMKz9pWU&ab_channel=JeremyHoward) - Jeremy Howard covers foundational concepts, evaluations of GPT-4, and practical applications of modern language models.
- [Spotify‚Äôs AI Voice Translation Pilot Means Your Favorite Podcasters Might Be Heard in Your Native Language](https://newsroom.spotify.com/2023-09-25/ai-voice-translation-pilot-lex-fridman-dax-shepard-steven-bartlett/) - Spotify pilots a new feature using AI to translate podcasts while retaining the original speaker's voice and style.


---

## üîñ License

All content in this repository is shared under the MIT License. Please refer to the LICENSE file for more details.
