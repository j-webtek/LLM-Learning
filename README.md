# 📘 LLM-Learning 

Welcome to LLM-Learning! Dive into a curated collection of intriguing research papers, innovative projects, and insightful documents from the world of Large Language Models and beyond.

---

## 🚀 Quick Navigation

1. [Visual Diagrams](#📊-visual-diagrams)
2. [Research Papers](#📄-research-papers)
3. [Projects](#🛠️-projects)
4. [Links & Articles](#🔗-links--articles)
5. [License](#🔖-license)

---

## 📊 Visual Diagrams

For a pictorial insight:
- [Data to LLM Processes](Data%20to%20LLM%20Processes.pdf)
- [Foundation Models](Foundation%20Models.pdf)
- [LLM Stack](LLM%20Stack.pdf)
- [Multi_Models](Multi_Models.pdf)

---

## 📄 Research Papers

Dive deep into the world of research with these enlightening papers:

### 🗓️ September 22, 2023:

- **Large Language Models for Compiler Optimization**
  - 📜 [Read the Paper](https://arxiv.org/pdf/2309.07062.pdf)
  - 📝 A study on the use of Large Language Models for code optimization.

- **PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions**
  - 📜 [Read the Paper](https://arxiv.org/pdf/2308.12261.pdf)
  - 📝 A method that transforms natural language task descriptions into deployable models.

- **Precise Zero-Shot Dense Retrieval without Relevance Labels**
  - 📜 [Read the Paper](https://aclanthology.org/2023.acl-long.99.pdf)
  - 📝 Introduction of Hypothetical Document Embeddings (HyDE) for zero-shot dense retrieval.

- **AnnoLLM: Making Large Language Models to Be Better**
  - 📜 [Read the Paper](https://www.semanticscholar.org/reader/70da4fb798a86cbe8cad96c27ced0415885bbd9d)
  - 📝 Large language models (LLMs) like GPT-3.5 can serve as effective crowdsourced annotators when given sufficient guidance and example demonstrations.

- **Want To Reduce Labeling Cost? GPT-3 Can Help**
  - 📜 [Read the Paper](https://www.semanticscholar.org/reader/70da4fb798a86cbe8cad96c27ced0415885bbd9d)
  - 📝 The study explores using GPT-3 as a cost-effective data labeler for training other models in NLP tasks.

- **DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions**
  - 📜 [Read the Paper](https://aclanthology.org/2023.acl-long.573.pdf)
  - 📝 The study introduces the task of recommending datasets based on a short natural language description of a research idea.

---

## 🛠️ Projects

### 💡 Model Optimization & Fine-Tuning

- 🌟 **Cleanlab** - An aid for cleaning data and labels in ML datasets. [Check it out](https://github.com/cleanlab/cleanlab).
- 🌟 **LLaMa-Adapter** - A lightweight adaptation method for fine-tuning Instruction-following and Multi-modal LLaMA models. [Learn more](https://github.com/OpenGVLab/LLaMA-Adapter).
- 🌟 **axolotl** - A tool designed to streamline the fine-tuning of AI models. [Check it out](https://github.com/OpenAccess-AI-Collective/axolotl).

### 🌐 Multimodal Models & Applications

- 🌟 **Awesome-Multimodal-Large-Language-Models** - A curated list of Multimodal Large Language Models. [Explore here](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models).
- 🌟 **DreamLLM** - A learning framework focusing on synergy between multimodal comprehension and creation. [Dive in](https://github.com/RunpeiDong/DreamLLM).
- 🌟 **NExT-GPT** - The first end-to-end MM-LLM for text, image, video, and audio. [Discover more](https://github.com/NExT-GPT/NExT-GPT).

### 🛍️ Collections & Repositories

- 🌟 **LargeLanguageModelsProjects** - A collection of llama models in different configurations. [Explore](https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Chat%20with%20Multiple%20Documents/Chat_with_Multiple_Documents_Llama2_OpenAI_Chroma_comp.ipynb).
- 🌟 **LiteLLM** - Manages inputs to the provider's completion and embedding endpoints. [Discover](https://github.com/BerriAI/litellm).
- 🌟 **AutoGPT** - A modular toolkit for AI agents. [Explore on GitHub](https://github.com/Significant-Gravitas/AutoGPT).
- 🌟 **localGPT** - Interact with documents locally ensuring data privacy. [Check it out on GitHub](https://github.com/PromtEngineer/localGPT).
- 🌟 **LLM-Finetuning-Hub** - Resources for finetuning LLMs tailored to specific use cases. [Learn more on GitHub](https://github.com/georgian-io/LLM-Finetuning-Hub).

---

## 🔗 Links & Articles

### 🌌 General AI Insights

- [Why Open Source AI Will Win](https://varunshenoy.substack.com/p/why-open-source-ai-will-win) - The debate between open-source and closed-source AI models.
- [RAG is more than just embedding search](https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/) - Potential of Retrieval Augmented Generation (RAG) in LLMs.
- [Emerging Architectures for LLM Applications](https://a16z.com/emerging-architectures-for-llm-applications/) - The article presents an architecture for applications using large language models (LLMs).
- [Practical insights and best practices for Fine Tuned LLM based use cases for Governed Enterprises](https://3ai.in/practical-insights-and-best-practices-for-fine-tuned-llm-based-use-cases-for-governed-enterprises/) - Aditya Khandekar's article on 3AI discusses best practices for deploying Large Language Models (LLMs) in enterprise settings.
- [Essential Guide to Foundation Models and Large Language Models](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404) - Foundation models explained.
- [Why You (Probably) Don’t Need to Fine-tune an LLM](https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/) - This article guides those building applications with Large Language Models (LLMs), emphasizing that while many consider fine-tuning LLMs to enhance performance, there are often simpler and more effective alternatives.

### 🛠️ Tools & Databases

- [Guide to Chroma DB | A Vector Store for Generative AI LLMs](https://www.analyticsvidhya.com/blog/2023/07/guide-to-chroma-db-a-vector-store-for-your-generative-ai-llms/) - An introduction to Chroma DB, a vector database for LLMs.
- [Build AI search into your applications](https://www.elastic.co/elasticsearch/elasticsearch-relevance-engine) - Introduction to the Elasticsearch Relevance Engine™ for AI-based search.
- [Llama API](https://python.langchain.com/docs/integrations/chat/llama_api) - This notebook shows how to use LangChain with LlamaAPI.
- [Embedchain](https://docs.embedchain.ai/quickstart) - Start building LLM-powered bots in under 30 seconds.

### 📚 Tutorials & How-Tos

- [Building a Knowledge base for custom LLMs](https://cismography.medium.com/building-a-knowledge-base-for-custom-llms-using-langchain-chroma-and-gpt4all-950906ae496d) - How to build a knowledge base for custom LLMs.
- [Fine-Tuning LLMs: LoRA or Full-Parameter?](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) - A comparison of full-parameter fine-tuning and LoRA for Llama 2 models.
- [Building a Scalable Pipeline for LLMs and RAG](https://ai.plainenglish.io/building-a-scalable-pipeline-for-large-language-models-and-rag-an-overview-7cb93a03f657) - Constructing a scalable pipeline for LLMs and RAG.
- [How to run a llama, alpaca, vicuna REST API for AI Discord Bot, Part 1](https://www.youtube.com/watch?v=PMFf9FwPN70&ab_channel=Janek) - How to set up a llama.cpp python binding server to host an API for LLM and how to create a python script for discord bot.

### 📈 Research & Innovations

- [Anand Katti](https://www.linkedin.com/posts/anand-katti-4278637_ai-genai-llm-activity-7097479600368160768-jNzj/) - LLMOps enhances the MLOps framework by introducing LLM-specific tasks.
- [An Introduction to LLMOps: Operationalizing and Managing Large Language Models using Azure ML](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/an-introduction-to-llmops-operationalizing-and-managing-large/ba-p/3910996) - Large language models (LLMs) like GPT-4 have transformed natural language processing with their superior performance, but their real-world deployment requires a systematic approach called LLMOps.

---

## 🔖 License

All content in this repository is shared under the MIT License. Please refer to the LICENSE file for more details.

---

This is your updated README. You can copy this content and paste it directly into your README file.
